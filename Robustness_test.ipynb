{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9928f624-88fc-4749-8dc0-28e8d79d56a1",
   "metadata": {},
   "source": [
    "## Robustness (beyond adversarial) with INV test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c276eda6-642f-43c1-a47f-eab03f84109b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp-models\n",
      "  Downloading allennlp_models-2.10.1-py3-none-any.whl (464 kB)\n",
      "Requirement already satisfied: datasets in d:\\anaconda\\lib\\site-packages (from allennlp-models) (1.11.0)\n",
      "Requirement already satisfied: torch<1.13.0,>=1.7.0 in d:\\anaconda\\lib\\site-packages (from allennlp-models) (1.10.2)\n",
      "Collecting word2number>=1.1\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\n",
      "Collecting conllu==4.4.2\n",
      "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting py-rouge==1.1\n",
      "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: nltk>=3.6.5 in d:\\anaconda\\lib\\site-packages (from allennlp-models) (3.7)\n",
      "Collecting allennlp<2.11,>=2.10.1\n",
      "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: h5py>=3.6.0 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (3.6.0)\n",
      "Collecting huggingface-hub>=0.0.16\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: pytest>=6.2.5 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (7.1.1)\n",
      "Collecting base58>=2.1.1\n",
      "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting lmdb>=1.2.1\n",
      "  Downloading lmdb-1.4.1-cp39-cp39-win_amd64.whl (105 kB)\n",
      "Requirement already satisfied: dill>=0.3.4 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.3.6)\n",
      "Collecting traitlets>5.1.1\n",
      "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
      "Collecting torchvision<0.14.0,>=0.8.1\n",
      "  Downloading torchvision-0.13.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: scipy>=1.7.3 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (1.9.2)\n",
      "Collecting cached-path<1.2.0,>=1.1.3\n",
      "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: filelock<3.8,>=3.3 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (3.6.0)\n",
      "Collecting wandb<0.13.0,>=0.10.0\n",
      "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting requests>=2.28\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: transformers<4.21,>=4.1 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (4.2.2)\n",
      "Requirement already satisfied: typer>=0.4.1 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.62 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (4.64.0)\n",
      "Collecting termcolor==1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.1.96 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.1.97)\n",
      "Collecting fairscale==0.4.6\n",
      "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.21.4 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (1.23.4)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (3.20.3)\n",
      "Collecting more-itertools>=8.12.0\n",
      "  Downloading more_itertools-9.1.0-py3-none-any.whl (54 kB)\n",
      "Collecting tensorboardX>=1.2\n",
      "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
      "Collecting scikit-learn>=1.0.1\n",
      "  Downloading scikit_learn-1.2.2-cp39-cp39-win_amd64.whl (8.4 MB)\n",
      "Requirement already satisfied: spacy<3.4,>=2.1.0 in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (3.3.2)\n",
      "Requirement already satisfied: sacremoses in d:\\anaconda\\lib\\site-packages (from allennlp<2.11,>=2.10.1->allennlp-models) (0.0.43)\n",
      "Collecting huggingface-hub>=0.0.16\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "Requirement already satisfied: boto3<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.21.32)\n",
      "Collecting google-cloud-storage<3.0,>=1.32.0\n",
      "  Downloading google_cloud_storage-2.8.0-py2.py3-none-any.whl (113 kB)\n",
      "Collecting rich<13.0,>=12.1\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in d:\\anaconda\\lib\\site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (0.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\anaconda\\lib\\site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.32 in d:\\anaconda\\lib\\site-packages (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.24.32)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in d:\\anaconda\\lib\\site-packages (from botocore<1.25.0,>=1.24.32->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\anaconda\\lib\\site-packages (from botocore<1.25.0,>=1.24.32->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.8.2)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in d:\\anaconda\\lib\\site-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.33.0)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.17.2-py2.py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\anaconda\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (4.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\anaconda\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\anaconda\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (4.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in d:\\anaconda\\lib\\site-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in d:\\anaconda\\lib\\site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (1.15.0)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\lib\\site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.21)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.1->allennlp-models) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.1->allennlp-models) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.1->allennlp-models) (4.1.1)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk>=3.6.5->allennlp-models) (1.1.0)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk>=3.6.5->allennlp-models) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk>=3.6.5->allennlp-models) (2022.3.15)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.0.16->allennlp<2.11,>=2.10.1->allennlp-models) (3.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (0.4.8)\n",
      "Requirement already satisfied: attrs>=19.2.0 in d:\\anaconda\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in d:\\anaconda\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in d:\\anaconda\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in d:\\anaconda\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in d:\\anaconda\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (1.2.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in d:\\anaconda\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (1.4.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from pytest>=6.2.5->allennlp<2.11,>=2.10.1->allennlp-models) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp-models) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.28->allennlp<2.11,>=2.10.1->allennlp-models) (3.3)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in d:\\anaconda\\lib\\site-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp<2.11,>=2.10.1->allennlp-models) (2.11.2)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn>=1.0.1->allennlp<2.11,>=2.10.1->allennlp-models) (2.2.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.4.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (61.2.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.9.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.6.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (1.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (5.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (1.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (0.7.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (8.0.17)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.6)\n",
      "Collecting torch<1.13.0,>=1.7.0\n",
      "  Downloading torch-1.12.1-cp39-cp39-win_amd64.whl (161.8 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda\\lib\\site-packages (from torchvision<0.14.0,>=0.8.1->allennlp<2.11,>=2.10.1->allennlp-models) (9.0.1)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in d:\\anaconda\\lib\\site-packages (from transformers<4.21,>=4.1->allennlp<2.11,>=2.10.1->allennlp-models) (0.9.4)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-win_amd64.whl (11 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in d:\\anaconda\\lib\\site-packages (from wandb<0.13.0,>=0.10.0->allennlp<2.11,>=2.10.1->allennlp-models) (5.8.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\n",
      "tensorflow-intel 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "jupyter-server 1.13.5 requires pywinpty<2; os_name == \"nt\", but you have pywinpty 2.0.2 which is incompatible.\n",
      "en-core-web-md 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.3.2 which is incompatible.\n",
      "constituent-treelib 0.0.5 requires spacy==3.4.1, but you have spacy 3.3.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: multiprocess in d:\\anaconda\\lib\\site-packages (from datasets->allennlp-models) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in d:\\anaconda\\lib\\site-packages (from datasets->allennlp-models) (2022.2.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\lib\\site-packages (from datasets->allennlp-models) (3.2.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in d:\\anaconda\\lib\\site-packages (from datasets->allennlp-models) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda\\lib\\site-packages (from datasets->allennlp-models) (3.8.1)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from datasets->allennlp-models) (1.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets->allennlp-models) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets->allennlp-models) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets->allennlp-models) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets->allennlp-models) (1.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets->allennlp-models) (1.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in d:\\anaconda\\lib\\site-packages (from ftfy->allennlp-models) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp<2.11,>=2.10.1->allennlp-models) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets->allennlp-models) (2021.3)\n",
      "Building wheels for collected packages: fairscale, termcolor, promise, word2number, pathtools\n",
      "  Building wheel for fairscale (PEP 517): started\n",
      "  Building wheel for fairscale (PEP 517): finished with status 'done'\n",
      "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307237 sha256=f6b9af23df6e61b4e186ef2a9fcd95f1dfe140faae6ed75422da121f9f284724\n",
      "  Stored in directory: c:\\users\\anaverageone\\appdata\\local\\pip\\cache\\wheels\\59\\b9\\f8\\08050f4b5734886ee64f25d1725e9c1e0a2bd19482b0385f25\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=a245ce76aa740cadc67ba709cece70e7e4ec95f59f776fe605b96745f81e2767\n",
      "  Stored in directory: c:\\users\\anaverageone\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=51187d81477bf77ffc4fbc87f07ca47d386c4283d51404129d891052b2291510\n",
      "  Stored in directory: c:\\users\\anaverageone\\appdata\\local\\pip\\cache\\wheels\\e1\\e8\\83\\ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for word2number (setup.py): started\n",
      "  Building wheel for word2number (setup.py): finished with status 'done'\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5583 sha256=4f646236f9c0df51079047f8568951b4bc3e824467a4b26c3c6813ce4e2dea10\n",
      "  Stored in directory: c:\\users\\anaverageone\\appdata\\local\\pip\\cache\\wheels\\a0\\4a\\5b\\d2f2df5c344ddbecb8bea759872c207ea91d93f57fb54e816e\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=411b3b77d8737322f6f272efc92a03c0090e4ef7cd0bc1a93024d7fd6764024f\n",
      "  Stored in directory: c:\\users\\anaverageone\\appdata\\local\\pip\\cache\\wheels\\b7\\0a\\67\\ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built fairscale termcolor promise word2number pathtools\n",
      "Installing collected packages: urllib3, requests, googleapis-common-protos, google-auth, smmap, google-api-core, joblib, google-resumable-media, google-cloud-core, gitdb, commonmark, torch, shortuuid, setproctitle, sentry-sdk, rich, promise, pathtools, huggingface-hub, google-cloud-storage, GitPython, docker-pycreds, wandb, traitlets, torchvision, termcolor, tensorboardX, scikit-learn, responses, more-itertools, lmdb, fairscale, cached-path, base58, word2number, py-rouge, ftfy, datasets, conllu, allennlp, allennlp-models\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.53.0\n",
      "    Uninstalling googleapis-common-protos-1.53.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.53.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.33.0\n",
      "    Uninstalling google-auth-1.33.0:\n",
      "      Successfully uninstalled google-auth-1.33.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.25.1\n",
      "    Uninstalling google-api-core-1.25.1:\n",
      "      Successfully uninstalled google-api-core-1.25.1\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 1.3.1\n",
      "    Uninstalling google-resumable-media-1.3.1:\n",
      "      Successfully uninstalled google-resumable-media-1.3.1\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 1.7.1\n",
      "    Uninstalling google-cloud-core-1.7.1:\n",
      "      Successfully uninstalled google-cloud-core-1.7.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.2\n",
      "    Uninstalling torch-1.10.2:\n",
      "      Successfully uninstalled torch-1.10.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.12\n",
      "    Uninstalling huggingface-hub-0.0.12:\n",
      "      Successfully uninstalled huggingface-hub-0.0.12\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 1.31.0\n",
      "    Uninstalling google-cloud-storage-1.31.0:\n",
      "      Successfully uninstalled google-cloud-storage-1.31.0\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.1.1\n",
      "    Uninstalling traitlets-5.1.1:\n",
      "      Successfully uninstalled traitlets-5.1.1\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.1.1\n",
      "    Uninstalling termcolor-2.1.1:\n",
      "      Successfully uninstalled termcolor-2.1.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.11.0\n",
      "    Uninstalling datasets-1.11.0:\n",
      "      Successfully uninstalled datasets-1.11.0\n",
      "Successfully installed GitPython-3.1.31 allennlp-2.10.1 allennlp-models-2.10.1 base58-2.1.1 cached-path-1.1.6 commonmark-0.9.1 conllu-4.4.2 datasets-2.10.1 docker-pycreds-0.4.0 fairscale-0.4.6 ftfy-6.1.1 gitdb-4.0.10 google-api-core-2.11.0 google-auth-2.17.2 google-cloud-core-2.3.2 google-cloud-storage-2.8.0 google-resumable-media-2.4.1 googleapis-common-protos-1.59.0 huggingface-hub-0.10.1 joblib-1.2.0 lmdb-1.4.1 more-itertools-9.1.0 pathtools-0.1.2 promise-2.3 py-rouge-1.1 requests-2.28.2 responses-0.18.0 rich-12.6.0 scikit-learn-1.2.2 sentry-sdk-1.19.1 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 tensorboardX-2.6 termcolor-1.1.0 torch-1.12.1 torchvision-0.13.1 traitlets-5.9.0 urllib3-1.26.15 wandb-0.12.21 word2number-1.1\n"
     ]
    }
   ],
   "source": [
    "#this cell of code you can comment out once you installed the following tools in your environment\n",
    "# pip install panda\n",
    "# pip install allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04948bf-8ed6-4153-a6d4-4ca43e95e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "from allennlp_models import pretrained\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import csv\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\anaverageone\\htlt_env\\Advanced_NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b24d251-fcf7-4290-b055-362ade40f150",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\ANAVER~1\\AppData\\Local\\Temp\\tmpzqw7b56g\\config.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\coref-spanbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\evaluate_rc-lerc.json as plain json\n",
      "lerc is not a registered model.\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\generation-bart.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\glove-sst.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\lm-masked-language-model.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\lm-next-token-lm-gpt2.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\mc-roberta-commonsenseqa.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\mc-roberta-piqa.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\mc-roberta-swag.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\nlvr2-vilbert-head.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\nlvr2-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-adversarial-binary-gender-bias-mitigated-roberta-snli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-binary-gender-bias-mitigated-roberta-snli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-decomposable-attention-elmo.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-esim.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-roberta-mnli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-roberta-rte.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\pair-classification-roberta-snli.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-bidaf-elmo.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-bidaf.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-naqanet.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-nmn.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\rc-transformer-qa.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\roberta-sst.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\semparse-nlvr.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\semparse-text-to-sql.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\semparse-wikitables.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-biaffine-parser.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-constituency-parser.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-srl-bert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\structured-prediction-srl.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\tagging-elmo-crf-tagger.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\tagging-fine-grained-crf-tagger.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\tagging-fine-grained-transformer-crf-tagger.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\ve-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\vgqa-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating D:\\ANACONDA\\Lib\\site-packages\\allennlp_models\\modelcards\\vqa-vilbert.json as plain json\n",
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\ANAVER~1\\AppData\\Local\\Temp\\tmphkawm5k2\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "# Loading BERT_based and BiLSTM models from AllenNLP tool package\n",
    "Bert_based = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n",
    "BiLSTM = pretrained.load_predictor(\"structured-prediction-srl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1195feaf-8bab-4734-b83f-fff882a54a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Concatenative Adversaries based on examples from en_ewt-up-train.conllu & en_ewt-up-test.conllu \n",
    "# 3 x Grammatical Sentences [structurally similar to answer + random sentence];\n",
    "# 2 x Gibberish [Append distracting text + Meaning-altering typos but does change the lables] \n",
    "\n",
    "sentences = [\n",
    "    \"What if POSTNL turned into DPD?\", \n",
    "    \"POSTNL is a nice delivery company.\",\n",
    "    \"Why would anybody else use it for anything else?\",\n",
    "    \"Was this the jar opener?\",\n",
    "    \"I get bloger, of course.\"\n",
    "]\n",
    "\n",
    "gold = [\n",
    "    [\"O\", \"O\", \"B-ARG1\", \"B-V\", \"O\", \"B-ARG2\", \"O\"],\n",
    "    [\"B-ARG1\", \"B-V\", \"O\", \"O\", \"O\", \"B-ARG2\", \"O\"],\n",
    "    [\"O\", \"O\", \"B-ARG0\",\"I-ARG0\", \"B-V\", \"B-ARG1\", \"O\", \"B-ARG2\", \"I-ARG2\", \"O\"],\n",
    "    [\"B-V\", \"B-ARG1\", \"O\", \"O\", \"B-ARG2\", \"O\"],\n",
    "    [\"B-ARG0\", \"B-V\", \"B-ARG1\", \"O\", \"B-ARGM-ADV\", \"O\", \"O\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6201ffb9-8a4a-4fff-a750-361e17ea1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that saves challenge data as csv file \n",
    "def create_challenge_data(outfile, test_samples, test_gold):\n",
    "    with open(outfile, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "    # write headers\n",
    "        writer.writerow([\"Sentence\", \"Gold\"])\n",
    "    # write each sentence and its corresponding gold labels\n",
    "        for i in range(len(test_samples)):\n",
    "            writer.writerow([test_samples[i], test_gold[i]])\n",
    "            \n",
    "# a function that prints out the failure rate of model on corresponding challenge data set \n",
    "def get_Pred_results(outfile, predictor_name, test_samples):\n",
    "    with open(outfile, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "    # write headers\n",
    "        writer.writerow([\"Sentence\", \"Arguments\"])\n",
    "    # Loop through sentences and arguements\n",
    "        for sent in test_samples:\n",
    "            output = predictor_name.predict(sentence=sent)\n",
    "        # for sentences with more than one predicate\n",
    "            if len(output[\"verbs\"]) > 1:\n",
    "                arguments = output[\"verbs\"][1][\"tags\"]\n",
    "                print(arguments)\n",
    "                writer.writerow([sent, arguments])\n",
    "        # for sentences with one predicte \n",
    "            elif len(output[\"verbs\"]) == 1:\n",
    "                arguments = output[\"verbs\"][0][\"tags\"]\n",
    "                print(arguments)\n",
    "                writer.writerow([sent, arguments])\n",
    "        # for sentences without verbs\n",
    "            else:\n",
    "                writer.writerow([sent, \"\"])\n",
    "                print('NaN')\n",
    "\n",
    "# a function that iterate through each token and find matches(both span & lable count as correct)\n",
    "def calculate_failure_rate(model_name, test_name, gold_arg, model_arg):\n",
    "    matches_counter = 0\n",
    "    token_counter = 0\n",
    "    for i, sent_arg in enumerate(gold_arg):\n",
    "        gold_list = ast.literal_eval(sent_arg)\n",
    "        pred_list = ast.literal_eval(model_arg[i])\n",
    "        for i in range(len(gold_list)):\n",
    "            if gold_list[i] != pred_list[i]:\n",
    "                token_counter += 1\n",
    "            else:\n",
    "                token_counter += 1\n",
    "                matches_counter += 1\n",
    "    failure_rate = (token_counter - matches_counter) / token_counter\n",
    "    print(f'{model_name} model yeilds a {failure_rate} failure rate for {test_name} test!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8dba428-1fc4-4778-a2aa-f6a2f2733525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call \"create_challenge_data\" to save challenge data as csv file(s)\n",
    "create_challenge_data('data/robust_file.csv', sentences, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "623a0f3c-bed3-4305-ab18-d3498e6c7553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'B-ARG1', 'B-V', 'B-ARG2', 'I-ARG2', 'O']\n",
      "['B-ARG1', 'B-V', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'O']\n",
      "['B-ARGM-PRP', 'B-ARGM-MOD', 'B-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'O']\n",
      "['B-V', 'B-ARG1', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'O']\n",
      "['O', 'B-V', 'B-ARG2', 'O', 'B-ARGM-ADV', 'I-ARGM-ADV', 'O']\n",
      "\n",
      "['O', 'O', 'B-ARG1', 'B-V', 'B-ARGM-DIR', 'I-ARGM-DIR', 'O']\n",
      "['B-ARG1', 'B-V', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'O']\n",
      "['B-ARGM-CAU', 'B-ARGM-MOD', 'B-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'O']\n",
      "['B-V', 'B-ARG1', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'O']\n",
      "['B-ARG0', 'B-V', 'B-ARG1', 'O', 'B-ARGM-DIS', 'I-ARGM-DIS', 'O']\n"
     ]
    }
   ],
   "source": [
    "# call \"get_Pred_results\" to save model predictions as csv file(s)\n",
    "get_Pred_results('data/Bertbased_robust.csv', Bert_based, sentences)\n",
    "print()\n",
    "get_Pred_results('data/BiLSTM_robust.csv', BiLSTM, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb320a-7b13-439d-88e4-6dffedca0ba3",
   "metadata": {},
   "source": [
    "### Evaluation (failure rate) on challenge data (token level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb965603-8f73-43cb-87ae-29a3e0cff86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data file and model predictions as pandas dataframe\n",
    "df_gold = pd.read_csv('data/robust_file.csv', sep=',')\n",
    "df_bert = pd.read_csv('data/Bertbased_robust.csv', sep=',')\n",
    "df_bilstm = pd.read_csv('data/BiLSTM_robust.csv', sep=',')\n",
    "# prepare argument labels for comparison \n",
    "bert_arg = df_bert['Arguments'].tolist()\n",
    "bilstm_arg = df_bilstm['Arguments'].tolist()\n",
    "gold_arg = df_gold['Gold'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d141d-3c54-4d59-8fbe-c9c02dd0537a",
   "metadata": {},
   "source": [
    "### Bert-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c88f83-7992-4d70-847d-1cb94cac63b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertbased model yeilds a 0.43243243243243246 failure rate for Robustness test!\n"
     ]
    }
   ],
   "source": [
    "# call the caculation function to compute failure rate for each model\n",
    "calculate_failure_rate('Bertbased', 'Robustness', gold_arg, bert_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec11f97-1858-4e63-95f7-bc64cfc20e51",
   "metadata": {},
   "source": [
    "### BiLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad150b61-cd46-4662-b09d-6204debd0158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM model yeilds a 0.40540540540540543 failure rate for Robustness test!\n"
     ]
    }
   ],
   "source": [
    "calculate_failure_rate('BiLSTM', 'Robustness', gold_arg, bilstm_arg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
